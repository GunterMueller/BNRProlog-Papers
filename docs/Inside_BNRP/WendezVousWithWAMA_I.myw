##	I. Conceptual and Historical

###	A. Origins of Prolog

Prolog is a theorem proving technology. As a theorem prover, it has two components: resolution and unification, and everything else is secondary. In this section I will give a brief review of these two ideas as they were originally developed by logicians, and how they have been subsequently evolved in the Prolog context. This genetic description serves to introduce a number of themes and concepts which will be mentioned later.

Resolution is the process of taking two clauses (which are disjunctions of "literals" and their negations), one containing `p` and one containing `~p`, and combining them and "cancelling" the `p` and `~p`:

`|p \/ q , ~p \/ r ==> q \/ r|`

The correctness of this should be obvious; it is equivalent to the transitivity of material implication since `|p -> q == ~p \/ q|`. The completeness of this as an inference rule was shown by Robinson in 1967; this simplified the construction of theorem provers because it meant that only a single inference rule needed to be implemented instead of a half dozen or more.

When clauses are such that they have only one positive literal (Horn clauses), resolution can be done particularly efficiently. In this case it is convenient to transform the disjunction into implicative form: `|p \/ ~q \/ ~r|` can be written as `|p :- q /\ r|`.

Unification can be described as the process of forming a "most general unifier" for two terms, but this requires some explanation of the origin of terms. In predicate calculus one must deal with the quantifiers `|(x)|` meaning "for all `x`" and `|!Ex|` meaning "there exists an `x`", and the order of occurrence and scoping of the quantifiers matters. In the 1920's it was discovered that one can simplify things a lot by replacing the use of existential quantifiers by Skolem functions, which can be thought of as descriptions which refer to (and in fact characterize) the entity whose existence was stated. For example, "everyone has a mother" which can be stated as `|(x) !Ey mother(x,y)|` can be rephrased (somewhat redundantly) as `|(x) mother(x, mother_of(x))|`. Note that `mother_of(x)` is shown as depending on `x`, since the existential quantifier was inside the scope of the universal quantifier. Using this trick one can eliminate all existential quantifiers from a clause, and then since only universal quantifiers are left (and their order is immaterial) the explicit universal quantifiers can be replaced by the convention that all variables are implicitly universally quantified.

When during resolution the `p` and `~p` are resolved, where the occurrences of `p` are such terms, two issues arise. One is that the resolvent (or derived clause) must be the most general consequence possible; this is the unification problem, to find the most general set of substitutions of variables in both terms that makes them equivalent. Thus unification is traditionally described theoretically as a pair of sets of substitutions, considered as being separate from the terms being unified. But to achieve high performance, it was found to be better to modify the terms in place as required to make them match. When the terms are being modified in situ, it is necessary that the original clauses (which we are not done with yet usually, since there may be other ways to resolve them) be unchanged; this is the copying problem. (since making a copy of the clauses before resolving is one solution). (The latter also reflects the fact that a universally quantified axiom in logic has no limit on the number of times it can be used.)

Thus some of the things in Prolog are the way they are because of this historical connection with theorem proving in the first order predicate calculus (FOPC), and this provides an historic rationale for these features and, at least for those who are sufficiently comfortable with predicate calculus, motivates the definitions. The academic logic programming community, however, has generally tended to regard the predicate calculus connection as a logical justification for Prolog, which is a different matter entirely. This is a view with which I have little sympathy, for several reasons. First- why should Prolog need "logical justification" at all? No one expects a "logical justification" for FORTRAN (fortunately!) or Algol or ADA; conventually one does not even really expect complete consistency in a programming language. Second- if a logic connection is desirable, there are alternative logics, such as intuitionist predicate calculus and modal logics, which are actually closer to Prolog than is the classical predicate calculus. Third- I know of no one who actually thinks habitually 'in predicate calculus terms when writing Prolog, and I suspect that when Prolog programmers say that Prolog is "logical" (and they do say this), it does not refer to this predicate calculus connection. Finally- for technical reasons discussed later, Prolog does not in fact provide FOPC theorem proving, and, furthermore, the evolution of Prolog has been decidely away from (rather than towards) being a better FOPC theorem prover.

In the early 1970's Alain Colmerauer realized that using a resolution theorem prover on Horn clauses was very much like using a programming language, if one thought of Horn clauses as procedures, and the terms in the body of the clause as procedure calls. This analogy produced Prolog -- *Pro*gramming in *Log*ic --and effectively the idea of "Logic Programming" was born. Curiously, essentially the same system was created several years earlier (ALVIN??) by people who thought they were doing algebra, so it is a bit of an historical accident that we have Logic Programming rather than Algebraic Programming -- is this another reason why the predicate calculus connection is not to be taken too seriously?)

The central fact of Prolog -- in my view -- is that at its heart is the interpretive ambiguity and tension, even conflict, between Prolog as "logic" and Prolog as programming language. I think that it is this tension which has fueled the dialectic process that has driven Prolog evolution. Later, for example, we will discuss how emphasis on the programming language side led to efficient implementation technolgies and how emphasis on the logic side led to extensions like constraint logic programming. Furthermore, the ambiguity has increased over the years as new interpretations of the same formalism have proliferated minimal model semantics, the modal interpretation, narrowing algebras, concurrent process interpretations, and so on.

 <#TableOfContents>

###	B. Early Prolog Implementation Technology

The first implementations of Prolog in the early 1970's, capable at first of doing a few dozens of logical inferences (resolutions) per second, gradually spread through the academic theorem proving and AI community as better implementations became available. In particular the very efficient implementations designed by Warren for the DEC-10 system (Edinburgh Prolog) became widely used and helped solidify the language syntax and the extended functionality of added primitives, although in retrospect perhaps prematurely. With the advances in hardware and software, performance in the megalip range is now readily available at the desktop. In this section I will trace some of the significant developments in implementation prior to the Warren Abstract Machine.

The implementations of this epoch (c. 1980), although differing somewhat in detail, are typically based on a few common mechanisms which had been developed by this time. These are worth describing because most of them are still with us today, although in modified form. It should also be noted that the Mac version of BNR Prolog (Version 3) used this technology, although in significantly modified form.

The first mechanism is that symbolic names (atomic names and the names of the Skolem functions) are stuffed (but only once) into a global common string area during tokenization, so that only pointers to names appear in the data structures, and hence the comparison of names can be done by pointer comparison during unification.

The copying problem is solved by means of *structure sharing*. This means that a clause contains a so-called *skeleton* of a term only, with variables replaced by consecutive numbers or *indexes*. A copy is made by creating an array of N new variables on the stack (an *environment*) and the *molecule* of `((skeleton_pointer, environment_pointer))` defines the term, the variable indexes acting as indexes into the environment. Thus each call (resolution) instance creates a new environment, but all such instances share the same data structure for the clause.

Unification, when restricted to logic variables, becomes the problem of efficiently and incrementally computing equivalence relations. There is a standard trick for doing this if there is a global transitive linear order relation which can be used to break the symmetry that causes loops. With all variables on the environment stack, the addresses of variables provides such an ordering, so when a variable A is unified with a variable B, one can set A to point to B whenever A has a higher(lower) address. A variable when accessed can therefore be the start of a chain of variables which eventually terminates since the addresses are monotonically decreasing(increasing). Whether one uses higher or lower depends on which way the stack grows- if the stack grows positively, the higher (i.e., later) address should point to the lower (earlier), to eliminate dangling references when the stack is popped. The initial value of a variable (and the termination condition for variable chains) must either be an invalid address, and typically something easy to check for (0,-1), or alternatively, a self-reference.

Non-determinism and backtracking is handled by two related mechanisms, the *choicepoint* and the *trail*, which were borrowed from transaction processing systems. The choicepoint is a snapshot of the critical internal state of the "prolog engine" (state variables, including the top of the environment stack and which clause it is about to do (current clause)) just before beginning the unification of a clause head. Restoring the state variables (which discards anything placed on the stack) and then skipping the current clause provides an efficient backtracking mechanism. Any logical failure, e.g., a mismatch during unification, invokes this mechanism. In order to deal with variables older than the last choicepoint which may have been bound in the failed branch, an undo trail is used. This is a stack in which the address of any variable older than the top of the environment stack at the creation of the last choicepoint ("critical address") is recorded when it is bound. When a choicepoint is reinstated, the trail is used to undo any bindings made since that choicepoint was created. As a result of trailing, a variable chain should never be "short-circuited" in memory since eliding intermediate variables in a chain could cause the trail mechanism to fail to undo an equivalence when it should.

Prolog has no need for a special list construct, since one can easily be built from any symbol (e.g., `;`) as infix list operator and a special symbol (e.g., `nil`) for the empty list. However, in order to support a nice syntax (using "[.,.]") and to speed the processing on lists, it was usual to build in a version of LISP-like lists into Prolog systems. This has had several unfortunate results. Syntactically, it badly overworks "," which was already doing double duty as a separator between arguments in functors and calls and representing "and" in bodies, and it also means that there are many different ways to syntactically express the same list (using "[.,.]" versus "|" and "(..)"). Semantically, it is also not quite right since it permits ill-formed lists (`[x,y|2]`). It has also encouraged to some extent a LISP-like style in which lists tend to be over-used. These issues have been resolved rather better in BNR Prolog, but the weight of history can not be denied.

This is perhaps a good place to introduce the "occurs check" problem. According to the laws of classical logic, when a variable `X` is to be bound to a functor `f(Y,Z ... )`, it is necessary that `f` not depend on `X`. However, the check for this changes unification from being a fast (linear time) to being a very slow operation, and this makes Prolog non-viable as a computation language. Hence, implementations have almost without exception omitted this precaution. Although theoretically speaking this makes Prolog a flawed theorem prover for classical logic; more important, as a programming language its practical consequence is that if such a case should happen to arise in a computation, it will generate a *cyclic structure* in memory, which in most older systems will cause an abnormal termination somewhere else in the code downstream of the problem, with little or no indication of where the problem originated. This sort of problem, although fortunately apparently rare, can require a great deal of effort to track down when it does occur, and is a most undesirable feature. Most new Prolog systems since about 1986 (e.g. BNR Prolog, VM Prolog, SICSTUS, Seimens-Nixdorf, .. ) have instituted cyclic structure unification, i.e., they permit cyclic structures to form but do not trip over them later (quite in defiance of classical logic).

Finally, it should be mentioned that memory management was (and still is) a major concern with Prolog, and in many problems memory rather than speed may be the critical resource. The structure sharing approach, although minimizing the actual data structures kept on the stack, turned out to be a rather poor candidate for garbage collection, because it is difficult to remove an environment until *all* references to *any* of its variables have disappeared. Other memory saving devices, such as *tail recursion optimization* and *last call optimization*, although doable, are so only with great difficulty and at some cost in speed.

 <#TableOfContents>

###	C. Evolution of Prolog: Directions

By the early 1980's Prolog technology was established in a number of Universities and research centres (mostly in Europe and Japan) and commercial packages were beginning to appear. Implementations were sufficiently powerful (a few thousand logical inferences per second on PCs, a few tens of thousands on main frames) that interesting things could be done with it. Apart from general agreement that everyone would prefer more speed and smaller memory requirements, there was a considerable range of opinions about what else was wrong/right about it as a language.

At the risk of over-simplifying to the point of caricature, we can discern several major views. The simplest perhaps was the view represented by the commercial vendors and their practical customers, that speed (and to a lesser extent memory) were the only significant issues. Responses to this analysis varied from introducing thousands of primitives (one for every occasion and most especially one for "naive reverse") to changing the language into something quite different, (as Borlund did with Turbo Prolog), to introducing decent compiler technology (as described below) as was done by Quintus and the other major vendors.

The view of the logicians (quite rightly) was that Prolog was a badly flawed implementation of a completely uninteresting subset of predicate calculus, but one could use it to write a *real* theorem prover for FOPC. In fact, Prolog technology has been used very successfully to construct full FOPC theorem provers. (One such system is that developed by Dr. Spencer of U.N.B. who helped develop XMS Prolog.) Curiously, however, full FOPC theorem provers have not as yet turned out to be very useful except in very special circumstances, for reasons which remain tantalizingly obscure.

Some parts of the AI community (especially in North America) seemed to feel that Prolog was deficient because it didn't have (a) semantic nets, (b) object orientation (with multiple inheritance!), or (c) non-monotonic reasoning (as in OPS5?); the rest apparently simply didn't like it because it wasn't LISP, didn't originate at MIT or Stanford, or they didn't approve of backtracking on general principles (i.e., because it was used in PLANNER?). Perhaps they just could not imagine working without the 25Mbytes of LISP code needed to provide a high level environment on a LISP machine, or couldn't imagine throwing out the thousands of man years that had been invested in producing it, or the years it had taken to become familiar with it. Whatever the reasons, the AI community and its industrial counterparts in North America have been very slow to adopt Prolog compared with the situation in Europe.

Another large contingent (especially in Europe and Japan) seemed to believe in one or more of the following: (a) Prolog is too slow on a uniprocessor, so to make it go faster one needs to use many processors; (b) logic as a programming language does not assume sequential processing, so a logic programming language is inherently easy to parallelize; (c) it is possible to interpret a tail recursive predicate as a process, and variables and lists of variables as communication processes, so Prolog syntax can be used to write fine-grained multitasking operating systems. A decade later it is possible to say that the combination (a) and (b) can work, but one must give up the sequential features of Prolog backtracking and cut in favour of a "purer" logical language, while the combination of (a) and (c), on the other hand, seem generally to require a detailed operational understanding in order to achieve termination, and hence moves one away from a strict logical interpretation.

The last major direction, and the one of greatest relevance to BNR Prolog, has been that of Constraint Logic Programming. This could be construed as the response of those that believed that the major problem with Prolog was that it couldn't do arithmetic. This was initiated in the early 1980's by A. Colmerauer in Marseille, who realized that unification could be regarded as the solution to a set of "simultaneous equations" in term space, regarded as a free algebra. By substituting other algebraic structures (permitting equation solving) for the standard one, one could thereby generalize the idea of Prolog. In particular, in Prolog II the space of terms was widened to include cyclic terms ("rational trees"). This has several important consequences: one is that it liberates Prolog from its prior intellectual dependence on classical logic. Second, it sidesteps the issue of the "occurs check" problem, since the conditions that the occurs check was supposed to prevent are no longer problematic. Third, it permits the direct handling of problems whose natural data structure is a graph. In Prolog III, additional, but separate, solvers were added for boolean propositional calculus (boolean unification) and string concatenation.

Prolog III also provided an exact (rational) relational arithmetic for linear systems, where any non-linear bits are deferred until they become linear. This was implemented by incorporating an incremental and variable precision version of the simplex method for linear programming into the system. The use of exact arithmetic ensured that only sound results were computed, but raised the typical costs from O(n^3^) to O(n^5^) for a matrix inversion. A similar approach was taken in CLP(R) from IBM, but used floating point arithmetic, with the consequence that it is more practical but unsound.

Yet another historical stream was pioneered by the CHIP system, which was based on finite domains and integer arithmetic, and proved very useful at solving combinatorial and integer programming problems. Finally, BNR Prolog (Version 3) introduced the use of relational interval arithmetic, which provides a sound general (but weak) method of handling a wide class of arithmetic problems. With its later developement (CLP(BNR)) under Version 4, this methodology was found to subsume boolean unfication and finite domain methods and (when augmented with Prolog code) the linear methods as well (but not as efficiently), and could be used effectively in problems of mixed type.

 <#TableOfContents>

###	D. A Brief History of BNR Prolog

BNR Prolog has been evolving for about eight years, and therefore the reasons for many of its features (and dis-features) are either mainly or partly historical. For this reason it is useful to know a little about its technical history.

The forerunner of BNR Prolog was XMS Prolog, which was developed in 1986-87 within the XMS Project at BNR by William Older and Bruce Spencer. The first four versions of XMS Prolog were toy implementations of a structure-sharing interpreter written in BNR Pascal, but were valuable learning exercises. Some features, such as the "State Space" and context stack were already present at this early date. In 1987 the interpreter was totally redesigned and reimplemented in 68000 Assembler by W. Older to become "Version 5" of XMS Prolog. The handling of cyclic structures, breadth-first unification, variable functors, and variadic structures all date from this period.

BNR Prolog developed from XMS Prolog during 1988-89. Much of the Version 5 XMS Prolog core structure was retained, but ported into UCSD Pascal to run on Macs. Variable name retention was added at this time as well as `freeze`, and the syntax was changed radically to bring in it into conformance with the evolving Prolog standards. Also added was the relational interval subsystem based on the ideas of Dr. John Cleary of the U. of Calgary. A major portion of the work was aimed at adapting the system from a conventional command line interpreter, to a relatively complete graphics-oriented programming environment with high level access to many of the Mac capabilities.

The development team consisted of William Older, John Rummell, Susan Antoft, Rick Snapper, Bijan Farrahi, Peter Hoddinott, Andre Vellino, Al Sary, and Marianne Morin, all under the direction of Rick Workman. Many contributions were made by other people during this period and in the next couple of years. In particular, Peter Cashin, the director of CRL, initiated (among other things) the Panels subsystem which was developed further by Rick Workman.

The Mac versions of BNR Prolog (and the associated manuals) were distributed to many in the academic community and other researchers around the world from 1989 to 1992. Several universities, such as Karlsruhe in Germany and Laval in Quebec, have adopted it as a teaching language at one time or another, so that many thousands of students have come into contact with it in various places. The interval arithmetic system has attracted much interest in the constraint programming community, and the technology has been copied several times: Interlog (Dassault Electronique, Fr.), ILOG Solver (Fr.), 1994 release of VM Prolog (IBM, Paris), and very likely by future Prolog IV (Marseille) and Prince (an Esprit project), as well as in several non-commercial systems.

The Unix versions of BNR Prolog originated in 1991 with William Older's design for a WAM-style compiler extended for BNR Prolog and its corresponding byte-coded interpreter. A wholly new core system based on this design was implemented in C in 1992-93 by John Rummell, and an alternate interpreter in 68K Assembler was done by W. Older. A team of developers under Jean Jervis developed the X-system interface in 1993 and reimplemented the GUI tools on top. A greatly extended new design for the relational interval arithmetic subsystem, renamed CLP(BNR), was developed by William Older and Frederic Benhamou and implemented in 1993 and subsequently upgraded in 1994.

 <#TableOfContents>

###	E. The Warren Abstract Machine

The WAM, or Warren Abstract Machine, was designed by Warren in the mid -1980's to improve the speed and memory performance of Prolog implementations, and still forms the basis of most commercial Prolog implementations. It is based on the "Abstract Machine" compilation strategy, that is, to define an abstract machine whose instruction architecture is ideally suited to the compilation problem at hand. The abstract machine can then be interpreted, emulated, compiled away, or even implemented in hardware, to give a range of performance options.

The WAM design makes use of many of the mechanisms described earlier, such as the handling of variables and the trail. There is one difference: the WAM requires that an unbound variable is coded as self-reference, so that its value can be copied and still point back at its original location. The WAM strategy is to introduce a new stack (called the "heap"), and construct any possibly long lived data structures on the heap, so that all short-life data can be managed more effectively in a manner similar to that of a conventional language. (This is the same strategy as found in Pascal.) Clauses are copied (logically before unification) to the heap, which is only popped during backtracking. The fundamental trick is to let unification get "ahead" of the copy process whenever possible, and this is accomplished by having instructions which operate in two different "modes": in "read mode" they compare an existing structure against constants or variables, while in "write mode" they construct new structures on the heap. The mode can only be determined at run time, as it depends on how the clause is called (its instantiation pattern), so the key is to define the opcodes in such an abstract way that they can be emitted correctly by the compiler without knowing which mode they will run in.

The WAM design achieves speed mainly by eliminating unneeded work: elimination of unnecessary choicepoints and variable initialization, elimination of tag checking on the first use of a variable, partial evaluation of unification in various special cases, flattening of the top level of structure, and an efficient argument passing mechanism. To support this, the compiler distinguishes between the first use and subsequent uses of variables, and many opcodes come in a "-val" and a cheaper "-var" version. (The savings appears to be about a factor of 2-5 between a very efficient interpreter and a WAM byte-code interpreter.)

The WAM is also designed to make very efficient use of memory, with a simple and efficient mechanisms to pop environments when they are no longer needed, to "trim" variables off the end of environments when they are no longer needed, and to provide "last call optimization" and "tail recursion optimization" (which depends crucially on the argument passing mechanism) which can pop environments just before the final call in a clause. In order to support these functions and to maximize their impact, the compiler distinguishes between "temporary" variables (which do not need to be kept in an environment) and "permanent" variables (which do), and numbers permanent variables starting at the last call in the clause body (to maximize the amount of environment trimming that can be done). Last call optimization is done by having different opcodes for the last call in the body and popping the environment just before. It is important to recognize that since all data structures are on the heap as individual pieces, garbage collection of the heap becomes a more effective tool than it was under structure sharing, and hence the various environment saving mechanisms become crucial, since it is the environments that contain most of the external pointers into the heap and thus control primarily what is/is not "garbage".

There are two "safety" issues that complicate the WAM architecture and the compiler. Both have their origin in the use of permanent variables which are located in (ie initialized in) an environment but which may then be passed as arguments into a call. One problem is that such variables (uninstantiated) can never be copied into a heap structure (since heap structures must never reference the shorterlived environments). The second problem, is that these variables (if still uninstantiated) must be copied into the heap on the final call of a body, since last call optimization will deallocate the environment and leave a dangling reference. Since such variables can be passed as arguments, a compile time inferencing algorithm is needed to propagate information about the safety of variables, and special (much more expensive) opcodes are needed to deal with the possibly unsafe ones.

Another problematic issue worth mentioning has to do with *un*initialized variables in environments. As mentioned earlier, the use of such variables is one of the major time-saving devices in compiled Prolog, since not only does one skip the cost of initialization but the subsequent cost of checking the tag. It can only be used in compiler based systems, since it requires that the first use (in order of processing) of any variable be identified and a special opcode --*one which does not look at the current contents of the variable*-- be used. The current contents, typically leftovers from a previous backtracking cycle, are, of course, totally wrong (e.g., possibly refering to things which no longer exist) but syntactically correct. As mentioned earlier, the WAM design makes it possible to do effective garbage collection on the heap, but the presence of such uninitialized variables in the environments creates difficulties. At best they will cause unnecessary retention of data which should be discarded, but they cause additional problems because the standard global garbage collector must be written in such a way that it does not make mistakes when handling such a completely bogus - but superficially healthy- term in an environment.

 <#TableOfContents>
